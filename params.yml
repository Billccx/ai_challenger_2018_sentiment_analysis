# preprocess data parameters
max_len: 500
chinese_word_size: 195198 # add <num> to vocab
multi_categories: 20
num_sentiment: 4

# dataset parameters
batch_size: 1
# batch_size: 96
prefetch: 2
buffer_size: 120000

# embed / position vectors parameters
embed_size: 300
hidden_size: 100 # reduce word dimension to 100
dropout_rate: 0.1

# AttnConv parameters

## transformer parameters
num_attention_stacks: 3
num_heads: 4

## Convolution parameters
filter_size_list:
  - 10
  - 20
  - 50
  - 100
num_filters: 16  # num of feature maps generated by a filter
inner_dense_outshape:
  - 128

# Optimization parameters
label_smooth: true
epsilon: 0.1
use_regularizer: false
reg_const: 0.005

# learning rate parameters
learning_rate: 0.1
momentum: 0.9
# approximate 1 epoch when batch_size is 64 (trainset has 105,000 samples)
first_decay_steps: 1500
t_mul: 2.0  # t_mul times longer than previous time spent
m_mul: 1.0 # m_mul ** i * learning_rate when ith restart start
alpha: 0.0  # minimum lr during decay

# training parameters
train_steps: 100000